# AI Provider Configuration
# Set the default provider type: 'gemini', 'openrouter', or 'openapi'
AI_PROVIDER_TYPE=gemini

# Default model to use (provider-specific)
# Examples:
#   Gemini: gemini-3-flash-preview, gemini-2.5-flash, gemini-1.5-pro
#   OpenRouter: openai/gpt-4o, anthropic/claude-3.5-sonnet, google/gemini-pro-1.5
#   Custom OpenAPI: (depends on your endpoint)
AI_PROVIDER_MODEL=gemini-3-flash-preview

# ============================================
# Gemini API Configuration
# ============================================
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# ============================================
# OpenRouter API Configuration
# ============================================
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# ============================================
# Custom OpenAPI Configuration
# ============================================
# Base URL for your OpenAPI-compatible endpoint
# Examples:
#   - http://localhost:8000
#   - https://api.example.com/v1
#   - https://your-litellm-instance.com
# Note: Trailing slashes are automatically removed
OPENAPI_BASE_URL=

# API Key for your custom OpenAPI endpoint (optional if using custom auth)
OPENAPI_API_KEY=

# ============================================
# Legacy/Backward Compatibility
# ============================================
# The following are kept for backward compatibility but are deprecated:
# Use the provider-specific keys above instead
API_KEY=
